\mychapter{Precision of different computer methods}

In the process of improving precision in calculations, it is very important to understand the limits of the underlying hardware and software, and the cost of going over them.

\mysection{General introduction}

Representing real numbers in a computer is something that is quite challenging and possible only with important limitations. These are documented, but errors due to these limitations are very difficult to compute.

A good introduction to this topic is \cite{Goldberg}, this section describes the general principles described in it.

\mysubsection{Number representation}

\mysubsubsection{Computer representation}

Real numbers are represented by floating point numbers, that can be schematized as:
\begin{itemize}
\item a sign, encoded on 1 bit
\item an exponent $e$, encoded in several bits (11 on double precision)
\item a significand $s$, encoded in the rest of the bits (52 on double precision), these bits are called the mantissa
\end{itemize}

In arbitrary precision libraries, operations are made in software and thus doesn't depend too much on hardware representation, at the cost of an important speed loss. The system is the same though, but the size of the exponent and the mantissa is given by the user\footnote{Note that the mainstream GMP/MPFR doesn't allow to set truely arbitrary exponent}.

\mysubsubsection{Analytical representation}

Floating points numbers can be studied as being in the form $d.dd...dd\times\beta^e$, where $d.dd...dd$ is the significand and has p digits, $\beta$ is the base (assumed to be even) and $e$ the exponent.

To make the link with computer representations, we would have:
\begin{itemize}
\item $\beta=2$
\item $p$ equal to the number of bits in the mantissa
\item $d.dd...dd$ the mantissa, with d in base 2, for example 1.100110011001100
\item $e$ the exponent
\end{itemize}

\mysection{Different computer representations}

The representation size of a floating point number vary according to hardware architecture and user input. We will take into consideration here the most common types (which should cover 99.99\% of use cases). We describe here the IEEE-754 standard as implemented in C99, but there are strict equivalents in Fortran.

\mysubsection{A first approach}

In a first naive approach would give the following floating point representations (all values are in number of bits):

\begin{tabular}{|l|S|S|S|l|}
\hline
\multicolumn{1}{|c|}{\textbf{name}} & \multicolumn{1}{c|}{\textbf{total}} & \multicolumn{1}{c|}{\textbf{exponent}} & \multicolumn{1}{c|}{\textbf{mantissa}} & \multicolumn{1}{c|}{\textbf{keyword in C}}\\\hline
Single precision & 32 & 8 & 23 & \texttt{float} \\\hline
Double precision & 64 & 11 & 52 & \texttt{double} \\\hline
Quadruple precision & 128 & 15 & 112 & \texttt{long double} \\\hline
\end{tabular}

But as we will see, there are some discrepancies between this and reality, due to several implementation choices that we will detail in the next sections.

\mysubsection{Hardware limitation: x87 and SSE}

There is currently no mainstream hardware that can make operations on floating points with more than 80 bits, and this limitation tends to even increase. This is a very important topic to understand as it is highly non-obvious and implementation choices might seem very strange.

First we can distinguish between two floating point computing hardwares:
\begin{enumerate}
\item[x87 FPU] This is the historical hardware introduced by Intel, present in all x86 and x86\_64 computers. This hardware has 80-bits registers and thus cannot make more precise operations.
\item[CPU] All calculations done directly with the CPU are different and use 64-bit registers, and are thus less precise.
\end{enumerate}

80-bit floating point representation follows the \emph{extended precision} of IEEE-754 standards and is composed of a 15-bit exponent and a 64-bits mantissa.

Compilers tend to use modern instructions sets such as SSE\TODO{}, AVX\TODO{} or NEON\TODO{}, which fall in the above \emph{CPU} category. On decent compilers, it is possible to decide between modern instruction sets, FPU or both\footnote{on \texttt{gcc}, the \texttt{-mfpmath} switch allows this, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86_002d64-Options.html} and \url{http://gcc.gnu.org/wiki/Math_Optimization_Flags}}. It is thus necessary to study this topic if you want to work on precision. By default, on x86 processors, the FPU is used, while modern instructions set are the default on x86\_64 processors.

It is important to note that on ARM processors (present on smartphones, tablets, etc.), floating point operations are done in a VFP, a kind of modified FPU. This VFP has no clear specification, but it seems not to have 80-bit registers, so smartphones and tablets fall on the \emph{CPU} category.

\mysubsection{Real life compiler's implementation}

The discrepancy between hardware and a naive float approach is naturally also present between the naive approach and the various implementations of compilers. Indeed, compilers tend to stick as close to hardware as possible (which is their role) in order to use as few costy software emulation as possible.

For instance, as we've seen, no hardware is capable of more than 80-bit calculations, thus hardware 128-bit calculation is impossible. We can also safely assume that users using the C99 keyword \texttt{long double} don't want software emulation. To solve this dilemma, compilers chose different options, the main things to know being summed up in the next paragraphs.

\mysubsubsection{\texttt{double} on FPU}

\texttt{double} keyword always uses 64-bit representation in memory, but when it comes to representation in (80-bit) FPU registers, two behaviours are possible:
\begin{enumerate}
\item[80-bit mode] calculations and intermediate values are made in 80-bit, which improves the precision of the calculations\footnote{but may lead in wrong comparaison results, see \url{http://gcc.gnu.org/wiki/x87note}}
\item[64-bit mode] the mantissa of intermediate values is rounded to 53 bits (thus giving double precision precision)\footnote{This is what the \texttt{-mpc64} option of gcc or the \texttt{/fp:precise} option of MSVC do, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html} and \url{http://msdn.microsoft.com/en-us/library/e7s85ffb.aspx}.}.
\end{enumerate}

\mysubsubsection{\texttt{long double} on FPU}

\texttt{long double} can have several representations:
\begin{enumerate}
\item[64-bit] this is the default on MSVC, \texttt{long double} being a synonym of \texttt{double}\footnote{see \url{http://msdn.microsoft.com/en-us/library/9cx8xs15.aspx}}
\item[96-bit] this is the default on gcc. In this case, 16 bits are not used in calculations in 80-bit mode.
\item[128-bit] this is the case on gcc with the \texttt{-m128bit-long-double}\footnote{see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html}}. 48 bits are not used in calculations.
\end{enumerate}

For the sake of completeness, it's important to notice here that gcc allows \texttt{\_\_float80} type which is 80-bit long, and thus fits perfectly in FPU registers. This can be quite useful if you want to optimize on FPU. The GNU Fortran also has \texttt{KIND=10} option that does the same\footnote{see \url{http://gcc.gnu.org/onlinedocs/gfortran/KIND-Type-Parameters.html}}.

\mysubsubsection{\texttt{long double} on CPU}

The x86\_64 ABI\footnote{\url{}} states that \texttt{long double} has intermediate values and calculations is in extended precision (80-bit), all operations being performed on the FPU; so no CPU (SSE/AVX) code will be used with the keyword \texttt{long double}.

The ARM Development Tools\footnote{\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0067d/BABFCGFC.html}} state that \texttt{long double} is 64-bit long, and thus a synonym of double.

\mysubsubsection{Summing up}

We can thus construct the following table summing up the previous paragraphs, giving some name conventions we'll use later. These names are just convenient and (though largely the same) not identical to IEEE-754 standard. The number of bits cells are in the form $x$/$y$, where $x$ is the number of bits used in memory and $y$ the number of bits used in calculations and intermediate values.

\begin{tabu} to \linewidth{|X[2]|X[c]|X[4]|}
\hline
\rowfont[c]{\bfseries} Name & Nb. of bits & Name in C
\\\hline
Simple precision & 32/32 & \texttt{float} with \texttt{-mpc32} option\\\hline
Mixed simple precision & 32/80 & \texttt{float} \\\hline
Double precision & 64/64 & \texttt{double}, \texttt{long double} on ARM and modern instruction sets (SSE, AVX). If on FPU, must be set in 64-bit mode.\\\hline
Extended double precision & 80/80 & \texttt{\_\_float80} (extension of gcc, FPU only) \\\hline
Mixed double precision & 64/80 & in 80-bit mode FPU: \texttt{double}, \texttt{long double} on MSVC \\\hline
Triple precision & 96/80 & \texttt{long long} on x86 (computations with FPU)\\\hline
Quadruple precision & 128/80 & \texttt{long long} on x86\_64 (computations with FPU)\\\hline
\end{tabu}

This table is another view of the same data:

\begin{tabu} to \linewidth{|X|X[c]|X[c]|X[c]|X[c]|X[c]|}
\hline
\rowfont[c]{\bfseries} C keyword & FPU (64-bit) & FPU (80-bit) & Modern & MSVC
\\\hline
\texttt{float} & 32/64 & 32/80 & 32/32 & no difference\\\hline
\texttt{double} & 64/64 & 64/80 & 64/64 & no difference \\\hline
\texttt{\_\_float80} & error & 80/80 & error & error \\\hline
\texttt{long double} & 96/80 or 128/80 & 96/80 or 128/80 & idem \texttt{double} on ARM, mapped to FPU with others & idem \texttt{double} \\\hline
\end{tabu}

% http://gcc.gnu.org/onlinedocs/gcc/Floating-Types.html
% http://gcc.gnu.org/wiki/x87note
% http://gcc.gnu.org/wiki/Math_Optimization_Flags
% http://gcc.gnu.org/onlinedocs/gfortran/KIND-Type-Parameters.html
